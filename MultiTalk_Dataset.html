<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MultiTalk Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Dataset
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/IHP_Dataset">
              IHP Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Foggy_KITTI_Dataset">
              Foggy KITTI Dataset
            </a>
            <a class="navbar-item" href="">
              Fewshot Object Detection Benchmark
            </a>
            <a class="navbar-item" href="">
              Large Vision Models Competency Benchmark
            </a>
            <a class="navbar-item" href="">
              Jeju Autonomous Driving Dataset (v1)
            </a>
            <a class="navbar-item" href="">
              Jeju Autonomous Driving Dataset (v2)
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Common-sense_Sentence_Dataset">
              Common-sense Sentence Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/GYM-DMC-CDIL_Dataset">
              GYM/DMC-CDIL Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Robot_Navigation_Dataset">
              Robot Navigation Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/BEAF_Dataset">
              BEAF Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/MultiTalk_Dataset">
              MultiTalk Dataset
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MultiTalk Dataset</h1>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">Postech</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/postech-ami/MultiTalk/blob/main/MultiTalk_dataset/README.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/MultiTalk_dataset_teaser.png" height="100%">
      <h2 class="subtitle has-text-centered">
        MultiTalk dataset is a dataset designed for training multilingual 3D talking heads, which generates 3D talking heads from speeches in diverse languages. The dataset was introduced in our paper "MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset".
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Recent studies in speech-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input speech in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. To enhance expressiveness and multilingual capability of speech-driven 3D talking head generation model, we collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. It contains 293,812 clips with a resolution of 512x512, a frame rate of 25 fps, and an average duration of 5.19 seconds per clip. The dataset shows a balanced distribution across languages, with each language representing between 2.0% and 9.7% of the total. With our proposed dataset, the model performs robust lip synchronization performance across diverse languages, capturing the unique mouth movements associated with each language.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">

    <!-- <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Key Application</h2>
          <p>
            <code>Smart Surveillance System</code>
          </p>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Primary Data Type</h2>
          <p>
            <code>Training images</code>, <code>Test images</code>, <code>Train annotations.json</code>, <code>Test annotations.json</code>
          </p>
        </div>
      </div>
    </div> -->

    <!-- Dataset Information. -->
    <h2 class="title is-3">Dataset Information</h2>
    <!-- Key Application. -->
    <h3 class="title is-4">Key Application</h3>
    <div class="content has-text-justified">
      <p>
        <code>Speech-Driven 3D Talking Head Generation</code>
      </p>
    </div>
    <!--/ Key Application. -->
    <!-- Primary Data Type. -->
    <h3 class="title is-4">Primary Data Type</h3>
    <div class="content has-text-justified">
      <p>
        <code>Video</code>, <code>Transcript</code>, <code>3D mesh vertices</code>
      </p>
    </div>
    <!--/ Primary Data Type. -->
    <!-- Data Function. -->
    <h3 class="title is-4">Data Function</h3>
    <div class="content has-text-justified">
      <p>
        Training, testing
      </p>
    </div>
    <!--/ Data Function. -->
    <!-- Dataset Characteristics. -->
    <h3 class="title is-4">Dataset Characteristics</h3>
    <div class="content has-text-justified">
      <p>
        <li>Total number of clips: 293,812</li>
        <li>Total number of languages: 20</li>
        <li>Total duration: 423.2 hours</li>
        <li>Image resolution: 512x512</li>
        <li>Average duration per clip: 5.19 seconds</li>
      </p>
    </div>
    <!--/ Dataset Characteristics. -->
    <!-- Labels. -->
    <h3 class="title is-4">Labels</h3>
    <p>
      Video name, youtube id, start and end times in the original video, bounding box, language, transcript are in ./annotations/{arabic,catalan,croatian,czech,dutch,english,french,german,greek,hindi,italian,japanese,mandarin,polish,portuguese,russian,spanish,thai,turkish,ukrainian}.json
    </p>
    <pre><code>
      {
        "QrDZjUeiUwc_0":  // video name
        {
            "youtube_id": "QrDZjUeiUwc",                                // youtube id
            "duration": {"start_sec": 302.0, "end_sec": 305.56},        // start and end times in the original video
            "bbox": {"top": 0.0, "bottom": 0.8167, "left": 0.4484, "right": 0.9453},  // bounding box
            "language": "czech",                                        // language
            "transcript": "já jsem v podstatě obnovil svůj list z minulého roku"      // transcript
        },
        "QrDZjUeiUwc_1":  
        {
            "youtube_id": "QrDZjUeiUwc",                                
            "duration": {"start_sec": 0.12, "end_sec": 4.12},        
            "bbox": {"top": 0.0097, "bottom": 0.55, "left": 0.3406, "right": 0.6398},  
            "language": "czech",                                       
            "transcript": "ahoj tady anička a vítejte u dalšího easycheck videa"      
        }
        "..."
        "..."
    
      }
    </code></pre>
    <p>
      <li>You can pass the languages you want to download as arguments to the script. If you want to download all 20 languages, run the following script.</li>
      <pre><code>
        sh dataset.sh arabic catalan croatian czech dutch english french german greek hindi italian japanese mandarin polish portuguese russian spanish thai turkish ukrainian
      </code></pre>
    </p>
    <!--/ Labels. -->
    <!-- Directory Structure. -->
    <h3 class="title is-4">Directory Structure</h3>
    <p>
      After downloading, the folder structure will be as below. Each language folder contains the .mp4 videos.
    </p>
    <pre><code>
      ${ROOT}
      ├── multitalk_dataset        # MultiTalk Dataset
      │   ├── arabic
      │   │   ├── O-VJXuHb390_0.mp4
      │   │   ├── O-VJXuHb390_1.mp4
      │   │   ├── ...
      │   │   └── ...             
      │   ├── catalan                                        
      │   ├── ...          
      │   └── ...                 
      └── raw_video              # Original videos (you can remove this directory after downloading)
          ├── arabic              
          ├── catalan                          
          ├── ...          
          └── ...
    </code></pre>
      <!--/ Directory Structure. -->
    <!--/ Dataset Information. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citing Datasets</h2>
    <p>
      If you use MultiTalk dataset in your research, please use the following BibTeX entry.
    </p>
    <pre><code>
      @article{sung2024multitalk,
        title={MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset},
        author={Sung-Bin, Kim and Chae-Yeon, Lee and Son, Gihun and Hyun-Bin, Oh and Ju, Janghoon and Nam, Suekyeong and Oh, Tae-Hyun},
        journal={arXiv preprint arXiv:2406.14272},
        year={2024}
      }
    </code></pre>

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            @ 2025 <a href="https://etri-visualintelligence.github.io/">ETRI</a>. All rights reserved. <br>
            Designed by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
