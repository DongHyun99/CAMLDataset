<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CAML Dataset | ETRI">
  <meta name="keywords" content="CAML">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BEAF Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Dataset
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/IHP_Dataset">
              IHP Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Foggy_KITTI_Dataset">
              Foggy KITTI Dataset
            </a>
            <a class="navbar-item" href="">
              Fewshot Object Detection Benchmark
            </a>
            <a class="navbar-item" href="">
              Large Vision Models Competency Benchmark
            </a>
            <a class="navbar-item" href="">
              Jeju Autonomous Driving Dataset (v1)
            </a>
            <a class="navbar-item" href="">
              Jeju Autonomous Driving Dataset (v2)
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Common-sense_Sentence_Dataset">
              Common-sense Sentence Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/GYM-DMC-CDIL_Dataset">
              GYM/DMC-CDIL Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/Robot_Navigation_Dataset">
              Robot Navigation Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/BEAF_Dataset">
              BEAF Dataset
            </a>
            <a class="navbar-item" href="https://donghyun99.github.io/CAMLDataset/MultiTalk_Dataset">
              MultiTalk Dataset
            </a>
          </div>
        </div>
      </div>
  
    </div>
  </nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">BEAF Dataset</h1>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">Postech</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/postech-ami/BEAF?tab=readme-ov-file"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/BEAF_dataset_teaser.gif" height="100%">
      <h2 class="subtitle has-text-centered">
        BEAF is a dataset designed for evaluating the hallucination of visiln language models (VLMs). The dataset was introduced in our paper "BEAF"
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            The key idea of our BEAF benchmark is manipulating visual scene information and designing the metrics based on the model's answer changes along the scene changes.
          </p>
          <p>
            Large vision language models (LVLMs) perceive the world through a combination of a visual encoder and large language models (LLMs). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and LLMs endow the high reasoning ability to LVLMs. It leads LVLMs to achieve high performance on wide benchmarks without fine-tuning, known as zero or few-shot capability of LLMs. However, recent studies show that LVLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from LVLMs. To enhance trustworthiness and better tackle the hallucination of LVLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is that we manipulate visual scene information by image editing models and design the metrics based on scene changes. This allows us to clearly assess whether LVLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize the correctness heatmap by virtue of our two-axis view: vision and text. Upon evaluating LVLMs with our dataset, we observed that our metrics can reveal different aspects of LVLM hallucination.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">

    <!-- <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Key Application</h2>
          <p>
            <code>Smart Surveillance System</code>
          </p>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Primary Data Type</h2>
          <p>
            <code>Training images</code>, <code>Test images</code>, <code>Train annotations.json</code>, <code>Test annotations.json</code>
          </p>
        </div>
      </div>
    </div> -->

    <!-- Dataset Information. -->
    <h2 class="title is-3">Dataset Information</h2>
    <!-- Key Application. -->
    <h3 class="title is-4">Key Application</h3>
    <div class="content has-text-justified">
      <p>
        <code>Evaluation of hallucination in VLMs</code>
      </p>
    </div>
    <!--/ Key Application. -->
    <!-- Primary Data Type. -->
    <h3 class="title is-4">Primary Data Type</h3>
    <div class="content has-text-justified">
      <p>
        <code>Image</code>, <code>Question</code>, <code>Answer</code>, <code>Additional metadata</code>
      </p>
    </div>
    <!--/ Primary Data Type. -->
    <!-- Data Function. -->
    <h3 class="title is-4">Data Function</h3>
    <div class="content has-text-justified">
      <p>
        Evaluation
      </p>
    </div>
    <!--/ Data Function. -->
    <!-- Dataset Characteristics. -->
    <h3 class="title is-4">Dataset Characteristics</h3>
    <div class="content has-text-justified">
      <p>
        <li>The number of images: 2,224</li>
        <li>The number of image-question pairs: 26,064</li>
      </p>
    </div>
    <!--/ Dataset Characteristics. -->
    <!-- Labels. -->
    <h3 class="title is-4">Labels</h3>
    <p>
      <li>Image name, question, GT answers, and additional metadata are in ./beaf_qna.json file</li>
    </p>
    <pre><code>
      [
          {
              "id": 0,
              "image": "COCO_val2014_000000001171.jpg",
              "question": "Is there a car in the image?",
              "orig_img": true,
              "removed_q": false,
              "gt": "yes"
          },
          {
              "id": 1,
              "image": "COCO_val2014_000000001171.jpg",
              "question": "Is there a train in the image?",
              "orig_img": true,
              "removed_q": true,
              "gt": "yes"
          },
          ...
      ]
    </code></pre>
    <br>
    <p>
      <li>The model output should be organized in a json file in the following format:</li>
    </p>
    <pre><code>
      [
        {"id": 0, "answer": "No."}, 
        {"id": 1, "answer": "Yes."}, 
        ... 
        {"id": 26063, "answer": "No."}
      ]
    </code></pre>
    <!--/ Labels. -->
    <!-- Directory Structure. -->
    <h3 class="title is-4">Directory Structure</h3>
    <pre><code>
      ./
      ├── COCO_val2014_000000001171.jpg
      ├── COCO_val2014_000000001171_00.png
      ├── COCO_val2014_000000001171_01.png
      ...
      ├── COCO_val2014_000000580294.png
    </code></pre>
      <!--/ Directory Structure. -->
    <!--/ Dataset Information. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citing Datasets</h2>
    <p>
      If you use this datasets in your research, please use the following BibTeX entry.
    </p>
    <pre><code>
      @inproceedings{yebin2024beaf,
        title     = {BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models},
        author    = {Ye-Bin, Moon and Hyeon-Woo, Nam and Choi, Wonseok and Oh, Tae-Hyun},
        booktitle = {European Conference on Computer Vision (ECCV)},
        year      = {2024},
      }
      
      @inproceedings{lin2014microsoft,
        title     = {Microsoft coco: Common objects in context},
        author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{'a}r, Piotr and Zitnick, C Lawrence},
        booktitle = {European Conference on Computer Vision (ECCV)},
        year      = {2014},
      }
      
      @inproceedings{Li-hallucination-2023,
        title     = {Evaluating Object Hallucination in Large Vision-Language Models},
        author    = {Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao and Ji-Rong Wen},
        booktitle = {The 2023 Conference on Empirical Methods in Natural Language Processing},
        year      = {2023},
      }
    </code></pre>

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            @ 2025 <a href="https://etri-visualintelligence.github.io/">ETRI</a>. All rights reserved. <br>
            Designed by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
